{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting sentence-transformers\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/45/18/1ec591befcbdb2c97192a40fbe7c43a8b8a8b3c89b1fa101d3eeed4d79a4/sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/75/d5/294a09a62bdd88da9a1007a341d4f8fbfc43be520c101e6afb526000e9f4/transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /root/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /root/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (2.3.0+cu121)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c6/29/044048c5e911373827c0e1d3051321b9183b2a4f8d4e2f11c08fcff83f13/scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from sentence-transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8e/ee/8a26858ca517e9c64f84b4c7734b89bda8e63bec85c3d2f432d225bb1886/scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/60/bf/cea0b9720c32fa01b0c4ec4b16b9f4ae34ca106b202ebbae9f03ab98cd8f/huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /root/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.40)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/29/52/84662b6636061277cb857f658518aa7db6672bc6d1a3f503ccd5aefc581e/regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d9/40/a6f75ea449a9647423ec8b6f72c16998d35aa4b43cb38536ac060c5c7bf5/safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.8/434.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/24/59/664121cb41b4f738479e2e1271013a2a7c9160955922536fb723a9c690b7/tokenizers-0.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /root/miniconda3/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Installing collected packages: threadpoolctl, scipy, safetensors, regex, joblib, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.26.2 joblib-1.4.2 regex-2024.9.11 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.2.1 threadpoolctl-3.5.0 tokenizers-0.20.1 transformers-4.46.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: tensorboard in /root/miniconda3/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard) (1.64.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard) (5.27.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting pandas\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/38/f8/d8fddee9ed0d0c0f4a2132c1dfcf0e3e53265055da8df952a53e7eaf178c/pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.26.0 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/11/c3/005fcca25ce078d2cc29fd559379817424e94885510568bc1bc53d7d5846/pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a6/ab/7e5f53c3b9d14972843a647d8d7a853969a58aecc7559cb3267302c94774/tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers\n",
    "%pip install tensorboard\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer   \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorboard as tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义数据集结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_list = {\n",
    "    \"E\" : 0,\n",
    "    \"I\" : 1,\n",
    "    \"N\" : 0, \n",
    "    \"S\" : 1,\n",
    "    \"F\" : 0,\n",
    "    \"T\" : 1,\n",
    "    \"J\" : 0,\n",
    "    \"P\" : 1\n",
    "}\n",
    "\n",
    "class MBTIDataset(Dataset):\n",
    "    def __init__(self, folder_path, mbti_type):\n",
    "        # mbti_type可以是\"EI\"/\"NS\"/\"FT\"/\"JP\"中的一个,表示二分类的类型\n",
    "        assert mbti_type in [\"EI\", \"NS\", \"FT\", \"JP\"]\n",
    "        self.type_idx = {\"EI\":0, \"NS\":1, \"FT\":2, \"JP\":3}[mbti_type]\n",
    "        \n",
    "        print(\"开始读取数据文件...\")\n",
    "        # 读取文件夹下所有csv文件并合并\n",
    "        import os\n",
    "        csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "        data_frames = []\n",
    "        for i, csv_file in enumerate(csv_files):\n",
    "            print(f\"正在读取第{i+1}/{len(csv_files)}个文件: {csv_file}\")\n",
    "            csv_path = os.path.join(folder_path, csv_file)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            data_frames.append(df)\n",
    "        data_sheet = pd.concat(data_frames, ignore_index=True)\n",
    "        print(f\"成功读取{len(csv_files)}个文件\")\n",
    "        \n",
    "        print(\"开始处理帖子数据...\")\n",
    "        self.posts = []\n",
    "        total = len(data_sheet['posts'])\n",
    "        for i, posts_50 in enumerate(data_sheet['posts']):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"处理进度: {i}/{total}\")\n",
    "            posts_50_list = posts_50.split(\"|||\")\n",
    "            if len(posts_50_list) < 50:\n",
    "                posts_50_list += [\"\"] * (50 - len(posts_50_list))\n",
    "            if len(posts_50_list) > 50:\n",
    "                posts_50_list = posts_50_list[:50]\n",
    "            self.posts.append(posts_50_list)\n",
    "        print(\"帖子数据处理完成\")\n",
    "                        \n",
    "        print(\"开始处理标签数据...\")\n",
    "        self.mbti_label = data_sheet['type']\n",
    "        for i in range(len(self.mbti_label)):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"标签处理进度: {i}/{len(self.mbti_label)}\")\n",
    "            # 只取对应维度的二分类标签\n",
    "            self.mbti_label[i] = check_list[self.mbti_label[i][self.type_idx]]\n",
    "            self.mbti_label[i] = torch.tensor(self.mbti_label[i])\n",
    "        print(\"标签处理完成\")\n",
    "        print(f\"数据集初始化完成,共{len(self.posts)}条数据\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"posts\" : self.posts[idx], \"type\" : self.mbti_label[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读取数据文件...\n",
      "正在读取第1/29个文件: reddit_mbti_chunked_0.csv\n",
      "正在读取第2/29个文件: reddit_mbti_chunked_1.csv\n",
      "正在读取第3/29个文件: reddit_mbti_chunked_11.csv\n",
      "正在读取第4/29个文件: reddit_mbti_chunked_12.csv\n",
      "正在读取第5/29个文件: reddit_mbti_chunked_13.csv\n",
      "正在读取第6/29个文件: reddit_mbti_chunked_14.csv\n",
      "正在读取第7/29个文件: reddit_mbti_chunked_2.csv\n",
      "正在读取第8/29个文件: reddit_mbti_chunked_3.csv\n",
      "正在读取第9/29个文件: reddit_mbti_chunked_4.csv\n",
      "正在读取第10/29个文件: reddit_mbti_chunked_5.csv\n",
      "正在读取第11/29个文件: reddit_mbti_chunked_6.csv\n",
      "正在读取第12/29个文件: reddit_mbti_chunked_7.csv\n",
      "正在读取第13/29个文件: reddit_mbti_chunked_8.csv\n",
      "正在读取第14/29个文件: reddit_mbti_chunked_9.csv\n",
      "正在读取第15/29个文件: reddit_mbti_chunked_15.csv\n",
      "正在读取第16/29个文件: reddit_mbti_chunked_16.csv\n",
      "正在读取第17/29个文件: reddit_mbti_chunked_17.csv\n",
      "正在读取第18/29个文件: reddit_mbti_chunked_18.csv\n",
      "正在读取第19/29个文件: reddit_mbti_chunked_19.csv\n",
      "正在读取第20/29个文件: reddit_mbti_chunked_20.csv\n",
      "正在读取第21/29个文件: reddit_mbti_chunked_21.csv\n",
      "正在读取第22/29个文件: reddit_mbti_chunked_22.csv\n",
      "正在读取第23/29个文件: reddit_mbti_chunked_23.csv\n",
      "正在读取第24/29个文件: reddit_mbti_chunked_24.csv\n",
      "正在读取第25/29个文件: reddit_mbti_chunked_25.csv\n",
      "正在读取第26/29个文件: reddit_mbti_chunked_26.csv\n",
      "正在读取第27/29个文件: reddit_mbti_chunked_27.csv\n",
      "正在读取第28/29个文件: reddit_mbti_chunked_28.csv\n",
      "正在读取第29/29个文件: reddit_mbti_chunked_29.csv\n",
      "成功读取29个文件\n",
      "开始处理帖子数据...\n",
      "处理进度: 0/283429\n",
      "处理进度: 1000/283429\n",
      "处理进度: 2000/283429\n",
      "处理进度: 3000/283429\n",
      "处理进度: 4000/283429\n",
      "处理进度: 5000/283429\n",
      "处理进度: 6000/283429\n",
      "处理进度: 7000/283429\n",
      "处理进度: 8000/283429\n",
      "处理进度: 9000/283429\n",
      "处理进度: 10000/283429\n",
      "处理进度: 11000/283429\n",
      "处理进度: 12000/283429\n",
      "处理进度: 13000/283429\n",
      "处理进度: 14000/283429\n",
      "处理进度: 15000/283429\n",
      "处理进度: 16000/283429\n",
      "处理进度: 17000/283429\n",
      "处理进度: 18000/283429\n",
      "处理进度: 19000/283429\n",
      "处理进度: 20000/283429\n",
      "处理进度: 21000/283429\n",
      "处理进度: 22000/283429\n",
      "处理进度: 23000/283429\n",
      "处理进度: 24000/283429\n",
      "处理进度: 25000/283429\n",
      "处理进度: 26000/283429\n",
      "处理进度: 27000/283429\n",
      "处理进度: 28000/283429\n",
      "处理进度: 29000/283429\n",
      "处理进度: 30000/283429\n",
      "处理进度: 31000/283429\n",
      "处理进度: 32000/283429\n",
      "处理进度: 33000/283429\n",
      "处理进度: 34000/283429\n",
      "处理进度: 35000/283429\n",
      "处理进度: 36000/283429\n",
      "处理进度: 37000/283429\n",
      "处理进度: 38000/283429\n",
      "处理进度: 39000/283429\n",
      "处理进度: 40000/283429\n",
      "处理进度: 41000/283429\n",
      "处理进度: 42000/283429\n",
      "处理进度: 43000/283429\n",
      "处理进度: 44000/283429\n",
      "处理进度: 45000/283429\n",
      "处理进度: 46000/283429\n",
      "处理进度: 47000/283429\n",
      "处理进度: 48000/283429\n",
      "处理进度: 49000/283429\n",
      "处理进度: 50000/283429\n",
      "处理进度: 51000/283429\n",
      "处理进度: 52000/283429\n",
      "处理进度: 53000/283429\n",
      "处理进度: 54000/283429\n",
      "处理进度: 55000/283429\n",
      "处理进度: 56000/283429\n",
      "处理进度: 57000/283429\n",
      "处理进度: 58000/283429\n",
      "处理进度: 59000/283429\n",
      "处理进度: 60000/283429\n",
      "处理进度: 61000/283429\n",
      "处理进度: 62000/283429\n",
      "处理进度: 63000/283429\n",
      "处理进度: 64000/283429\n",
      "处理进度: 65000/283429\n",
      "处理进度: 66000/283429\n",
      "处理进度: 67000/283429\n",
      "处理进度: 68000/283429\n",
      "处理进度: 69000/283429\n",
      "处理进度: 70000/283429\n",
      "处理进度: 71000/283429\n",
      "处理进度: 72000/283429\n",
      "处理进度: 73000/283429\n",
      "处理进度: 74000/283429\n",
      "处理进度: 75000/283429\n",
      "处理进度: 76000/283429\n",
      "处理进度: 77000/283429\n",
      "处理进度: 78000/283429\n",
      "处理进度: 79000/283429\n",
      "处理进度: 80000/283429\n",
      "处理进度: 81000/283429\n",
      "处理进度: 82000/283429\n",
      "处理进度: 83000/283429\n",
      "处理进度: 84000/283429\n",
      "处理进度: 85000/283429\n",
      "处理进度: 86000/283429\n",
      "处理进度: 87000/283429\n",
      "处理进度: 88000/283429\n",
      "处理进度: 89000/283429\n",
      "处理进度: 90000/283429\n",
      "处理进度: 91000/283429\n",
      "处理进度: 92000/283429\n",
      "处理进度: 93000/283429\n",
      "处理进度: 94000/283429\n",
      "处理进度: 95000/283429\n",
      "处理进度: 96000/283429\n",
      "处理进度: 97000/283429\n",
      "处理进度: 98000/283429\n",
      "处理进度: 99000/283429\n",
      "处理进度: 100000/283429\n",
      "处理进度: 101000/283429\n",
      "处理进度: 102000/283429\n",
      "处理进度: 103000/283429\n",
      "处理进度: 104000/283429\n",
      "处理进度: 105000/283429\n",
      "处理进度: 106000/283429\n",
      "处理进度: 107000/283429\n",
      "处理进度: 108000/283429\n",
      "处理进度: 109000/283429\n",
      "处理进度: 110000/283429\n",
      "处理进度: 111000/283429\n",
      "处理进度: 112000/283429\n",
      "处理进度: 113000/283429\n",
      "处理进度: 114000/283429\n",
      "处理进度: 115000/283429\n",
      "处理进度: 116000/283429\n",
      "处理进度: 117000/283429\n",
      "处理进度: 118000/283429\n",
      "处理进度: 119000/283429\n",
      "处理进度: 120000/283429\n",
      "处理进度: 121000/283429\n",
      "处理进度: 122000/283429\n",
      "处理进度: 123000/283429\n",
      "处理进度: 124000/283429\n",
      "处理进度: 125000/283429\n",
      "处理进度: 126000/283429\n",
      "处理进度: 127000/283429\n",
      "处理进度: 128000/283429\n",
      "处理进度: 129000/283429\n",
      "处理进度: 130000/283429\n",
      "处理进度: 131000/283429\n",
      "处理进度: 132000/283429\n",
      "处理进度: 133000/283429\n",
      "处理进度: 134000/283429\n",
      "处理进度: 135000/283429\n",
      "处理进度: 136000/283429\n",
      "处理进度: 137000/283429\n",
      "处理进度: 138000/283429\n",
      "处理进度: 139000/283429\n",
      "处理进度: 140000/283429\n",
      "处理进度: 141000/283429\n",
      "处理进度: 142000/283429\n",
      "处理进度: 143000/283429\n",
      "处理进度: 144000/283429\n",
      "处理进度: 145000/283429\n",
      "处理进度: 146000/283429\n",
      "处理进度: 147000/283429\n",
      "处理进度: 148000/283429\n",
      "处理进度: 149000/283429\n",
      "处理进度: 150000/283429\n",
      "处理进度: 151000/283429\n",
      "处理进度: 152000/283429\n",
      "处理进度: 153000/283429\n",
      "处理进度: 154000/283429\n",
      "处理进度: 155000/283429\n",
      "处理进度: 156000/283429\n",
      "处理进度: 157000/283429\n",
      "处理进度: 158000/283429\n",
      "处理进度: 159000/283429\n",
      "处理进度: 160000/283429\n",
      "处理进度: 161000/283429\n",
      "处理进度: 162000/283429\n",
      "处理进度: 163000/283429\n",
      "处理进度: 164000/283429\n",
      "处理进度: 165000/283429\n",
      "处理进度: 166000/283429\n",
      "处理进度: 167000/283429\n",
      "处理进度: 168000/283429\n",
      "处理进度: 169000/283429\n",
      "处理进度: 170000/283429\n",
      "处理进度: 171000/283429\n",
      "处理进度: 172000/283429\n",
      "处理进度: 173000/283429\n",
      "处理进度: 174000/283429\n",
      "处理进度: 175000/283429\n",
      "处理进度: 176000/283429\n",
      "处理进度: 177000/283429\n",
      "处理进度: 178000/283429\n",
      "处理进度: 179000/283429\n",
      "处理进度: 180000/283429\n",
      "处理进度: 181000/283429\n",
      "处理进度: 182000/283429\n",
      "处理进度: 183000/283429\n",
      "处理进度: 184000/283429\n",
      "处理进度: 185000/283429\n",
      "处理进度: 186000/283429\n",
      "处理进度: 187000/283429\n",
      "处理进度: 188000/283429\n",
      "处理进度: 189000/283429\n",
      "处理进度: 190000/283429\n",
      "处理进度: 191000/283429\n",
      "处理进度: 192000/283429\n",
      "处理进度: 193000/283429\n",
      "处理进度: 194000/283429\n",
      "处理进度: 195000/283429\n",
      "处理进度: 196000/283429\n",
      "处理进度: 197000/283429\n",
      "处理进度: 198000/283429\n",
      "处理进度: 199000/283429\n",
      "处理进度: 200000/283429\n",
      "处理进度: 201000/283429\n",
      "处理进度: 202000/283429\n",
      "处理进度: 203000/283429\n",
      "处理进度: 204000/283429\n",
      "处理进度: 205000/283429\n",
      "处理进度: 206000/283429\n",
      "处理进度: 207000/283429\n",
      "处理进度: 208000/283429\n",
      "处理进度: 209000/283429\n",
      "处理进度: 210000/283429\n",
      "处理进度: 211000/283429\n",
      "处理进度: 212000/283429\n",
      "处理进度: 213000/283429\n",
      "处理进度: 214000/283429\n",
      "处理进度: 215000/283429\n",
      "处理进度: 216000/283429\n",
      "处理进度: 217000/283429\n",
      "处理进度: 218000/283429\n",
      "处理进度: 219000/283429\n",
      "处理进度: 220000/283429\n",
      "处理进度: 221000/283429\n",
      "处理进度: 222000/283429\n",
      "处理进度: 223000/283429\n",
      "处理进度: 224000/283429\n",
      "处理进度: 225000/283429\n",
      "处理进度: 226000/283429\n",
      "处理进度: 227000/283429\n",
      "处理进度: 228000/283429\n",
      "处理进度: 229000/283429\n",
      "处理进度: 230000/283429\n",
      "处理进度: 231000/283429\n",
      "处理进度: 232000/283429\n",
      "处理进度: 233000/283429\n",
      "处理进度: 234000/283429\n",
      "处理进度: 235000/283429\n",
      "处理进度: 236000/283429\n",
      "处理进度: 237000/283429\n",
      "处理进度: 238000/283429\n",
      "处理进度: 239000/283429\n",
      "处理进度: 240000/283429\n",
      "处理进度: 241000/283429\n",
      "处理进度: 242000/283429\n",
      "处理进度: 243000/283429\n",
      "处理进度: 244000/283429\n",
      "处理进度: 245000/283429\n",
      "处理进度: 246000/283429\n",
      "处理进度: 247000/283429\n",
      "处理进度: 248000/283429\n",
      "处理进度: 249000/283429\n",
      "处理进度: 250000/283429\n",
      "处理进度: 251000/283429\n",
      "处理进度: 252000/283429\n",
      "处理进度: 253000/283429\n",
      "处理进度: 254000/283429\n",
      "处理进度: 255000/283429\n",
      "处理进度: 256000/283429\n",
      "处理进度: 257000/283429\n",
      "处理进度: 258000/283429\n",
      "处理进度: 259000/283429\n",
      "处理进度: 260000/283429\n",
      "处理进度: 261000/283429\n",
      "处理进度: 262000/283429\n",
      "处理进度: 263000/283429\n",
      "处理进度: 264000/283429\n",
      "处理进度: 265000/283429\n",
      "处理进度: 266000/283429\n",
      "处理进度: 267000/283429\n",
      "处理进度: 268000/283429\n",
      "处理进度: 269000/283429\n",
      "处理进度: 270000/283429\n",
      "处理进度: 271000/283429\n",
      "处理进度: 272000/283429\n",
      "处理进度: 273000/283429\n",
      "处理进度: 274000/283429\n",
      "处理进度: 275000/283429\n",
      "处理进度: 276000/283429\n",
      "处理进度: 277000/283429\n",
      "处理进度: 278000/283429\n",
      "处理进度: 279000/283429\n",
      "处理进度: 280000/283429\n",
      "处理进度: 281000/283429\n",
      "处理进度: 282000/283429\n",
      "处理进度: 283000/283429\n",
      "帖子数据处理完成\n",
      "开始处理标签数据...\n",
      "标签处理进度: 0/283429\n",
      "标签处理进度: 1000/283429\n",
      "标签处理进度: 2000/283429\n",
      "标签处理进度: 3000/283429\n",
      "标签处理进度: 4000/283429\n",
      "标签处理进度: 5000/283429\n",
      "标签处理进度: 6000/283429\n",
      "标签处理进度: 7000/283429\n",
      "标签处理进度: 8000/283429\n",
      "标签处理进度: 9000/283429\n",
      "标签处理进度: 10000/283429\n",
      "标签处理进度: 11000/283429\n",
      "标签处理进度: 12000/283429\n",
      "标签处理进度: 13000/283429\n",
      "标签处理进度: 14000/283429\n",
      "标签处理进度: 15000/283429\n",
      "标签处理进度: 16000/283429\n",
      "标签处理进度: 17000/283429\n",
      "标签处理进度: 18000/283429\n",
      "标签处理进度: 19000/283429\n",
      "标签处理进度: 20000/283429\n",
      "标签处理进度: 21000/283429\n",
      "标签处理进度: 22000/283429\n",
      "标签处理进度: 23000/283429\n",
      "标签处理进度: 24000/283429\n",
      "标签处理进度: 25000/283429\n",
      "标签处理进度: 26000/283429\n",
      "标签处理进度: 27000/283429\n",
      "标签处理进度: 28000/283429\n",
      "标签处理进度: 29000/283429\n",
      "标签处理进度: 30000/283429\n",
      "标签处理进度: 31000/283429\n",
      "标签处理进度: 32000/283429\n",
      "标签处理进度: 33000/283429\n",
      "标签处理进度: 34000/283429\n",
      "标签处理进度: 35000/283429\n",
      "标签处理进度: 36000/283429\n",
      "标签处理进度: 37000/283429\n",
      "标签处理进度: 38000/283429\n",
      "标签处理进度: 39000/283429\n",
      "标签处理进度: 40000/283429\n",
      "标签处理进度: 41000/283429\n",
      "标签处理进度: 42000/283429\n",
      "标签处理进度: 43000/283429\n",
      "标签处理进度: 44000/283429\n",
      "标签处理进度: 45000/283429\n",
      "标签处理进度: 46000/283429\n",
      "标签处理进度: 47000/283429\n",
      "标签处理进度: 48000/283429\n",
      "标签处理进度: 49000/283429\n",
      "标签处理进度: 50000/283429\n",
      "标签处理进度: 51000/283429\n",
      "标签处理进度: 52000/283429\n",
      "标签处理进度: 53000/283429\n",
      "标签处理进度: 54000/283429\n",
      "标签处理进度: 55000/283429\n",
      "标签处理进度: 56000/283429\n",
      "标签处理进度: 57000/283429\n",
      "标签处理进度: 58000/283429\n",
      "标签处理进度: 59000/283429\n",
      "标签处理进度: 60000/283429\n",
      "标签处理进度: 61000/283429\n",
      "标签处理进度: 62000/283429\n",
      "标签处理进度: 63000/283429\n",
      "标签处理进度: 64000/283429\n",
      "标签处理进度: 65000/283429\n",
      "标签处理进度: 66000/283429\n",
      "标签处理进度: 67000/283429\n",
      "标签处理进度: 68000/283429\n",
      "标签处理进度: 69000/283429\n",
      "标签处理进度: 70000/283429\n",
      "标签处理进度: 71000/283429\n",
      "标签处理进度: 72000/283429\n",
      "标签处理进度: 73000/283429\n",
      "标签处理进度: 74000/283429\n",
      "标签处理进度: 75000/283429\n",
      "标签处理进度: 76000/283429\n",
      "标签处理进度: 77000/283429\n",
      "标签处理进度: 78000/283429\n",
      "标签处理进度: 79000/283429\n",
      "标签处理进度: 80000/283429\n",
      "标签处理进度: 81000/283429\n",
      "标签处理进度: 82000/283429\n",
      "标签处理进度: 83000/283429\n",
      "标签处理进度: 84000/283429\n",
      "标签处理进度: 85000/283429\n",
      "标签处理进度: 86000/283429\n",
      "标签处理进度: 87000/283429\n",
      "标签处理进度: 88000/283429\n",
      "标签处理进度: 89000/283429\n",
      "标签处理进度: 90000/283429\n",
      "标签处理进度: 91000/283429\n",
      "标签处理进度: 92000/283429\n",
      "标签处理进度: 93000/283429\n",
      "标签处理进度: 94000/283429\n",
      "标签处理进度: 95000/283429\n",
      "标签处理进度: 96000/283429\n",
      "标签处理进度: 97000/283429\n",
      "标签处理进度: 98000/283429\n",
      "标签处理进度: 99000/283429\n",
      "标签处理进度: 100000/283429\n",
      "标签处理进度: 101000/283429\n",
      "标签处理进度: 102000/283429\n",
      "标签处理进度: 103000/283429\n",
      "标签处理进度: 104000/283429\n",
      "标签处理进度: 105000/283429\n",
      "标签处理进度: 106000/283429\n",
      "标签处理进度: 107000/283429\n",
      "标签处理进度: 108000/283429\n",
      "标签处理进度: 109000/283429\n",
      "标签处理进度: 110000/283429\n",
      "标签处理进度: 111000/283429\n",
      "标签处理进度: 112000/283429\n",
      "标签处理进度: 113000/283429\n",
      "标签处理进度: 114000/283429\n",
      "标签处理进度: 115000/283429\n",
      "标签处理进度: 116000/283429\n",
      "标签处理进度: 117000/283429\n",
      "标签处理进度: 118000/283429\n",
      "标签处理进度: 119000/283429\n",
      "标签处理进度: 120000/283429\n",
      "标签处理进度: 121000/283429\n",
      "标签处理进度: 122000/283429\n",
      "标签处理进度: 123000/283429\n",
      "标签处理进度: 124000/283429\n",
      "标签处理进度: 125000/283429\n",
      "标签处理进度: 126000/283429\n",
      "标签处理进度: 127000/283429\n",
      "标签处理进度: 128000/283429\n",
      "标签处理进度: 129000/283429\n",
      "标签处理进度: 130000/283429\n",
      "标签处理进度: 131000/283429\n",
      "标签处理进度: 132000/283429\n",
      "标签处理进度: 133000/283429\n",
      "标签处理进度: 134000/283429\n",
      "标签处理进度: 135000/283429\n",
      "标签处理进度: 136000/283429\n",
      "标签处理进度: 137000/283429\n",
      "标签处理进度: 138000/283429\n",
      "标签处理进度: 139000/283429\n",
      "标签处理进度: 140000/283429\n",
      "标签处理进度: 141000/283429\n",
      "标签处理进度: 142000/283429\n",
      "标签处理进度: 143000/283429\n",
      "标签处理进度: 144000/283429\n",
      "标签处理进度: 145000/283429\n",
      "标签处理进度: 146000/283429\n",
      "标签处理进度: 147000/283429\n",
      "标签处理进度: 148000/283429\n",
      "标签处理进度: 149000/283429\n",
      "标签处理进度: 150000/283429\n",
      "标签处理进度: 151000/283429\n",
      "标签处理进度: 152000/283429\n",
      "标签处理进度: 153000/283429\n",
      "标签处理进度: 154000/283429\n",
      "标签处理进度: 155000/283429\n",
      "标签处理进度: 156000/283429\n",
      "标签处理进度: 157000/283429\n",
      "标签处理进度: 158000/283429\n",
      "标签处理进度: 159000/283429\n",
      "标签处理进度: 160000/283429\n",
      "标签处理进度: 161000/283429\n",
      "标签处理进度: 162000/283429\n",
      "标签处理进度: 163000/283429\n",
      "标签处理进度: 164000/283429\n",
      "标签处理进度: 165000/283429\n",
      "标签处理进度: 166000/283429\n",
      "标签处理进度: 167000/283429\n",
      "标签处理进度: 168000/283429\n",
      "标签处理进度: 169000/283429\n",
      "标签处理进度: 170000/283429\n",
      "标签处理进度: 171000/283429\n",
      "标签处理进度: 172000/283429\n",
      "标签处理进度: 173000/283429\n",
      "标签处理进度: 174000/283429\n",
      "标签处理进度: 175000/283429\n",
      "标签处理进度: 176000/283429\n",
      "标签处理进度: 177000/283429\n",
      "标签处理进度: 178000/283429\n",
      "标签处理进度: 179000/283429\n",
      "标签处理进度: 180000/283429\n",
      "标签处理进度: 181000/283429\n",
      "标签处理进度: 182000/283429\n",
      "标签处理进度: 183000/283429\n",
      "标签处理进度: 184000/283429\n",
      "标签处理进度: 185000/283429\n",
      "标签处理进度: 186000/283429\n",
      "标签处理进度: 187000/283429\n",
      "标签处理进度: 188000/283429\n",
      "标签处理进度: 189000/283429\n",
      "标签处理进度: 190000/283429\n",
      "标签处理进度: 191000/283429\n",
      "标签处理进度: 192000/283429\n",
      "标签处理进度: 193000/283429\n",
      "标签处理进度: 194000/283429\n",
      "标签处理进度: 195000/283429\n",
      "标签处理进度: 196000/283429\n",
      "标签处理进度: 197000/283429\n",
      "标签处理进度: 198000/283429\n",
      "标签处理进度: 199000/283429\n",
      "标签处理进度: 200000/283429\n",
      "标签处理进度: 201000/283429\n",
      "标签处理进度: 202000/283429\n",
      "标签处理进度: 203000/283429\n",
      "标签处理进度: 204000/283429\n",
      "标签处理进度: 205000/283429\n",
      "标签处理进度: 206000/283429\n",
      "标签处理进度: 207000/283429\n",
      "标签处理进度: 208000/283429\n",
      "标签处理进度: 209000/283429\n",
      "标签处理进度: 210000/283429\n",
      "标签处理进度: 211000/283429\n",
      "标签处理进度: 212000/283429\n",
      "标签处理进度: 213000/283429\n",
      "标签处理进度: 214000/283429\n",
      "标签处理进度: 215000/283429\n",
      "标签处理进度: 216000/283429\n",
      "标签处理进度: 217000/283429\n",
      "标签处理进度: 218000/283429\n",
      "标签处理进度: 219000/283429\n",
      "标签处理进度: 220000/283429\n",
      "标签处理进度: 221000/283429\n",
      "标签处理进度: 222000/283429\n",
      "标签处理进度: 223000/283429\n",
      "标签处理进度: 224000/283429\n",
      "标签处理进度: 225000/283429\n",
      "标签处理进度: 226000/283429\n",
      "标签处理进度: 227000/283429\n",
      "标签处理进度: 228000/283429\n",
      "标签处理进度: 229000/283429\n",
      "标签处理进度: 230000/283429\n",
      "标签处理进度: 231000/283429\n",
      "标签处理进度: 232000/283429\n",
      "标签处理进度: 233000/283429\n",
      "标签处理进度: 234000/283429\n",
      "标签处理进度: 235000/283429\n",
      "标签处理进度: 236000/283429\n",
      "标签处理进度: 237000/283429\n",
      "标签处理进度: 238000/283429\n",
      "标签处理进度: 239000/283429\n",
      "标签处理进度: 240000/283429\n",
      "标签处理进度: 241000/283429\n",
      "标签处理进度: 242000/283429\n",
      "标签处理进度: 243000/283429\n",
      "标签处理进度: 244000/283429\n",
      "标签处理进度: 245000/283429\n",
      "标签处理进度: 246000/283429\n",
      "标签处理进度: 247000/283429\n",
      "标签处理进度: 248000/283429\n",
      "标签处理进度: 249000/283429\n",
      "标签处理进度: 250000/283429\n",
      "标签处理进度: 251000/283429\n",
      "标签处理进度: 252000/283429\n",
      "标签处理进度: 253000/283429\n",
      "标签处理进度: 254000/283429\n",
      "标签处理进度: 255000/283429\n",
      "标签处理进度: 256000/283429\n",
      "标签处理进度: 257000/283429\n",
      "标签处理进度: 258000/283429\n",
      "标签处理进度: 259000/283429\n",
      "标签处理进度: 260000/283429\n",
      "标签处理进度: 261000/283429\n",
      "标签处理进度: 262000/283429\n",
      "标签处理进度: 263000/283429\n",
      "标签处理进度: 264000/283429\n",
      "标签处理进度: 265000/283429\n",
      "标签处理进度: 266000/283429\n",
      "标签处理进度: 267000/283429\n",
      "标签处理进度: 268000/283429\n",
      "标签处理进度: 269000/283429\n",
      "标签处理进度: 270000/283429\n",
      "标签处理进度: 271000/283429\n",
      "标签处理进度: 272000/283429\n",
      "标签处理进度: 273000/283429\n",
      "标签处理进度: 274000/283429\n",
      "标签处理进度: 275000/283429\n",
      "标签处理进度: 276000/283429\n",
      "标签处理进度: 277000/283429\n",
      "标签处理进度: 278000/283429\n",
      "标签处理进度: 279000/283429\n",
      "标签处理进度: 280000/283429\n",
      "标签处理进度: 281000/283429\n",
      "标签处理进度: 282000/283429\n",
      "标签处理进度: 283000/283429\n",
      "标签处理完成\n",
      "数据集初始化完成,共283429条数据\n",
      "Data Preprocessing Done\n",
      "{'posts': ['sine na support the directors actors and other people behind the film', 'i just want a nes classic controller since i already have most of the games on wii wii u virtual console and would also like to use it for smm its disturbing and should be outright illegal for people to be selling these at over a 1000 markup asking 100 for a 10 controller', 'for sure ! also check out bjorn nylands youtube channel if you havent', 'as opposed to intp ? definitely and the way youve put yourself out there seems to confirm it the only problem is that my ni judges types based on mannerisms eg eye movement not just eye appearance if that makes sense ? so while it usually only takes a couple of seconds the person needs to be actively existing in my line of sight', 'i cant believe i had to get this far down to see this she clearly has some sort of anxiety disorder and is possibly being made to handle a crowded mallretail situation during the holidays now she needs to learn respect but being that shes young shes out of options its not like she can leave on her own she needs help not judgment', 'haha i wasnt even like that as a kid d me shy stare world lets do stuff ! me is it safe ? world no ! thats why we do it ! me ill be over here watching you guys have fun it is fun to watch yall tho d', 'did ms frizzle shrink liz again ? !', 'or start writing the body the thesis and conclusion can come after once the main ideas are developed since theyre primarily the synopsis and summary of the essay', 'whenever a family member brings it up to me ive started saying well ive been keeping an eye on the shelter and when they look at me funny i say because dogs are the only kids im interested in right now i got so sick of sil asking me when i was going to give her a niece that i finally started replying with when are you going to give me one ? she thinks its funny i cant win', 'i do think western women have issues but i think some of them are taken way out of proportion and blown up as to be horrible oppression as if it was equivalent to the 1800s things like manspreading wage gap mansplaining etc but since you asked about mens issues some include the likes of what said below others could be men being more likely to die in industrial accidentssuicidewar men being more likely to lose in custody battles false rape accusations lack of helphomes for men who have experience domestic', 'violencerape men being taken less seriously when coming forward about rape than woman are', 'im european so i know exactly what he means but in the context of this sub i thought that gif was appropriate this isnt just a sports sub or even a football sub its only nfl besides i think its a bot', 'i personally wish they hadnt uploaded the whole thing online i would really have wanted it but now they are making it hard for me to buy it since it doesnt feel very cost efficient', 'that one hell of a tumultuous emotional roller coaster best of wishes op that this can at least get better', 'gt buttergolem ich brech ab d', 'above all you must do everything you can to fight this false narrative of the russian boogyman you must stop ww3 you must be prepared to get up in the faces of politicians and their donors who would dare draw us into conflict with russia we are being lied to according to the us peace envoy for the united nations we need to call out the propaganda and demand the truth you must call out the conflicts of interest that exist within family members of those in power or have been in political office you need to', 'root out the corruption and meet it head on the dapl is an emerging issue that is deteriorating dapl is only one of many our dependence on oil is the greatest national security threat that drives so much suffering and death in such we must end the petrodollar and the cancer that is the carter doctorine if we can not count on or depend on our actual representation to fight against corporate thuggery due to corruption and conflicts of interest then we need to start organizing against those financial interests', 'and start protesting the donor class themselves start boycotts of their monopolies stop participating in their monopolies that might require sacrifice that might mean going to the store on the other side of town or paying more for somethings stop allowing the ever intrusive creep of corporate control into our personal lives however if our vote doesnt mean anything our dollars sure do and from recent history it seems crying on a corporate twitter page gets a better response than thousands of letters to', 'congress all we need to do is cause a 5 delta in their profit margins to turn heads we must end the accepted practice of the profiting off human suffering in all its forms this includes the tpp and other trade deals that aim to circumvent labor and environmental laws just because you cant have your plantation here because you lost the civil war and the abusive labor practices of the industrial revolution doesnt mean its ok to seek them overseas at the cost of the american worker we the people need to draft', 'a new declaration of independence from the oligarchic thugs that have gone too far the situation in this country has become far too much like that in the times before the revolution and the civil war we need to lay out our grievances and say enough is enough', 'i saw another earth floating around out there but i never watched it now ill definitely have to check it out !', 'when we went for her shots last week i mentioned the crap napping to the doc and told her i was worried about daycare will they ignore my sweet crying girl ? ! ? she told us that most babies sleep better at daycare because they realise mom isnt there so they might as well sleep making the day go by more quickly that gave me some hope', 'sorry youre going through that i know the feeling youre referring to and it sucks you cant change it so i think the best thing to do is to take mental note of the vibes you were getting at moments when you could have done something different use those mental notes in the future to help motivate different decisions i would chalk this up as a learning experience', 'might depend on locality but im happy to be corrected at any rate i always thought it was required but my place in stockbridge said they werent so they didnt maintain the ones in the units instead we should run a few doors down and grab the good ones off the wall', 'looks like some of my games at master rank', 'maybe not ptsd but they can be effected i remember being around 5 and forced onto roller coasters hating the experience and hating them ever since imagine as a kid who already has very few life experiences that something such as that trick could be their worst experience of their short life', 'in regard to battlefield why is there any reason to think the rest could be poor ? battlefield has had pretty high quality in recent iterations', 'well like it wasnt a punishment given to me but to my sister my sister btw is like the most pessimistic negative person you could ever meet no joke in fact my parents have told me about how i would end up cowering whenever she came into the room when i was younger and she is my little sister by 2 years not big sister ! because she would throw whatever toys i was playing with at my head full force anyway she was being real stroppy on holiday and didnt want to wear a dress so rather than be all calm and', 'stuff my mum got real angry and just ripped the dress off my sister right there and then in the apartment she didnt complain much more for the rest of the holiday', 'as an aside my sister the infp does not get along with her very well lol', 'hmm most maybe even all infps i know seem to feel a huge desire to be understood more accurately they tend to feel misunderstood or perhaps even worse completely overlooked or brushed off by others who cant easily follow their train of thought as i mentioned already i think most of them are justified in these feelings my first inclination is to advise recognizing that when others dont understand or ignore you that shows zero evidence that youre wrong worthless or stupid it just means youre on a different', 'wavelength than they are and different and wrong are not the same thing but in my experience intellectually knowing this fact doesnt solve much for the infp this is because they still want to have close connections with people and to be understood not necessarily to be right so i want to try another approach the infps i know who are happiest are the ones who give to others the very thing they are seeking understanding encouragement to be the best self they can be basically someone who will just listen and', 'not judge theyre usually incredibly good at this when they make the effort everyone needs this to an extent and when an infp helps others fill these needs they begin to feel valued and needed themselves and often find over time that some of those people although certainly not all are more inclined to reciprocate and appreciate them for who they are the biggest challenge with this is that it requires stepping out of the introverted safe comfort zone forgetting your own insecurities and focusing entirely on', 'someone else in spite of fears of how they perceive you at the same time thats part of what makes it work as insecurities are forgotten they begin to disappear so those are my thoughts keep in mind that im not an infp but an infj i share zero functions with infps so i may not fully understand whats going on under the hood and i may have biases that run counter to what an infp really wantsneeds this is just based on what i hope that i correctly understand so far best of luck !', 'you have a great point indeed', 'it all depends on the way i felt around him comfortable or not and the type of person he was asshole or nice i cant say ive ever dated someone who i thought was an extrovert', 'well okay we didnt have the sharpshooters stop him sothat was still nice of us', 'you could email pics to fred oster at vintage instruments he knows pretty everything about banjos and will happily give you info good honest dude hes on antiques roadshow from time to time he sold my dad a banjo in the early 70s and then another one in the late 90s bear in mind it could be hand made by the person who played it and sort of has that look to me people often bought the hardware and made everything else themselves', 'dont put up with her shit dont let slide the mental and physical abuse cause even if you do in the end she leaves you try to help her but give her clear limits and if she doesnt respect them leave shell figure her shit out and maybe youll end up together again after that and maybe not but at least you wont hate her oh and here are the winning loto number for next week', 'i know a guy whos sister died because his father shot her in the head while cleaning his gun', 'lanky is deraugatory you should go to jail', 'this would be something to see', 'aww you know you like a little whimsy in your life', 'the team that just took two out of three against us ? i dont rate our chances against the jays as highly as you do', 'or expecting them to be straight', 'what is this doing on a yard sale page ?', 'well you can complete a regular old mba part time as well we have a full time mba at our school but we also have a part time mba', 'tsm rolled rox in scrims confirmed', 'as soon as you uttered this rule 34 flew into the intertubes after it were done for now', 'i think it may have something to do with the cynical ultrarealist nerdview that a lot of the people have round here bunch of eggheads yall eggheads'], 'type': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "dataset = MBTIDataset('../data/', \"EI\")\n",
    "for i in range(len(dataset)):\n",
    "    if len(dataset[i][\"posts\"]) != 50:\n",
    "        print(f\"{i} error, length is {len(dataset[i]['posts'])}\")  \n",
    "    \n",
    "print(\"Data Preprocessing Done\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建嵌入模型和分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import math\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(RoPE, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        # x shape: [batch_size, seq_len, embedding_dim]\n",
    "        device = x.device\n",
    "        position = torch.arange(seq_len, device=device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.embedding_dim, 2, device=device) * (-math.log(10000.0) / self.embedding_dim))\n",
    "        \n",
    "        # 计算旋转角度\n",
    "        theta = position * div_term  # [seq_len, embedding_dim/2]\n",
    "        \n",
    "        # 分别计算sin和cos\n",
    "        sin = torch.sin(theta)  # [seq_len, embedding_dim/2] \n",
    "        cos = torch.cos(theta)  # [seq_len, embedding_dim/2]\n",
    "        \n",
    "        # 对输入进行旋转变换\n",
    "        x_even = x[..., 0::2]  # 偶数维度\n",
    "        x_odd = x[..., 1::2]   # 奇数维度\n",
    "        \n",
    "        # 旋转变换\n",
    "        x_rotated_even = x_even * cos - x_odd * sin\n",
    "        x_rotated_odd = x_even * sin + x_odd * cos\n",
    "        \n",
    "        # 交错合并\n",
    "        x_rotated = torch.zeros_like(x)\n",
    "        x_rotated[..., 0::2] = x_rotated_even\n",
    "        x_rotated[..., 1::2] = x_rotated_odd\n",
    "        \n",
    "        return x_rotated\n",
    "\n",
    "class MBTIClassifier(nn.Module):\n",
    "    def __init__(self, device=torch.device('cuda'), model_path='../model/models--sentence-transformers--all-MiniLM-L6-v2'):\n",
    "        super(MBTIClassifier, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.embedding_dim = 384\n",
    "        self.num_classes = 16\n",
    "        \n",
    "        self.embedding_model = SentenceTransformer(model_path, device=self.device)\n",
    "        self.rope = RoPE(self.embedding_dim)      \n",
    "        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embedding_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=768,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_encoder_layer,\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.transformer_decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.embedding_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=768,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            self.transformer_decoder_layer,\n",
    "            num_layers=4\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embedding_dim, self.num_classes),\n",
    "        )\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, batch_sentences_list):\n",
    "        # Preprocess each sentence in the batch\n",
    "        preprocessed_batch = []\n",
    "        for sentences_list in batch_sentences_list:\n",
    "            preprocessed_list = []\n",
    "            for sentence in sentences_list:\n",
    "                sentence = sentence.lower()\n",
    "                sentence = sentence.replace(\"  \", \" \")\n",
    "                preprocessed_list.append(sentence)\n",
    "            preprocessed_batch.append(preprocessed_list)\n",
    "        \n",
    "        # Flatten the batch to process with SentenceTransformer\n",
    "        flattened_sentences = [sentence for sentences_list in preprocessed_batch for sentence in sentences_list]\n",
    "        \n",
    "        # Encode all sentences in the batch\n",
    "        embeddings = self.embedding_model.encode(flattened_sentences, convert_to_tensor=True, device=self.device)\n",
    "        \n",
    "        # Reshape the embeddings to (batch_size, sequence_length, embedding_dim)\n",
    "        batch_size = len(preprocessed_batch)\n",
    "        sequence_length = len(preprocessed_batch[0])  # Assuming all lists have the same length\n",
    "        embeddings = embeddings.view(batch_size, sequence_length, -1)\n",
    "        embeddings = embeddings.permute(1, 0, 2)  # (sequence_length, batch_size, embedding_dim)\n",
    "        \n",
    "        # 对句子序列进行位置编码\n",
    "        embeddings = self.rope(embeddings, sequence_length)\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        \n",
    "        transformer_encoder_output = self.transformer_encoder(embeddings)\n",
    "\n",
    "        # 创建并对decoder输入进行位置编码\n",
    "        mbti_tensor = torch.ones(4, batch_size, self.embedding_dim).to(self.device)\n",
    "        mbti_tensor = self.rope(mbti_tensor, 4)\n",
    "\n",
    "        # Pass through the transformer decoder\n",
    "        transformer_output = self.transformer_decoder(mbti_tensor, transformer_encoder_output)\n",
    "        transformer_output = transformer_output.squeeze(0)\n",
    "        \n",
    "        # Pass through the feedforward network\n",
    "        output = self.ffn(transformer_output)\n",
    "        return output   \n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        \"\"\"\n",
    "        Sets the module in training mode.\n",
    "        \"\"\"\n",
    "        super(MBTIClassifier, self).train(mode)\n",
    "        self.embedding_model.train()\n",
    "        self.layer_norm.train()\n",
    "        self.transformer_encoder.train()\n",
    "        self.transformer_decoder.train()\n",
    "        self.ffn.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Sets the module in evaluation mode.\n",
    "        \"\"\"\n",
    "        super(MBTIClassifier, self).eval()\n",
    "        self.embedding_model.eval()\n",
    "        self.layer_norm.eval()\n",
    "        self.transformer_encoder.eval()\n",
    "        self.transformer_decoder.eval()\n",
    "        self.ffn.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import shutil\n",
    "import torch\n",
    "from torch.optim import AdamW, Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.8):\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    return random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, writer, global_step):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # posts is a list of 50 strings for each of the 16 samples\n",
    "        # We need to convert it to (16, 50) to fit the model\n",
    "        posts = batch['posts']  # Shape: (50, 16)\n",
    "        real_batch_size = len(posts[0])\n",
    "        posts_shape_fix = [[] for _ in range(real_batch_size)]\n",
    "        for i_, j_ in itertools.product(range(real_batch_size), range(50)):\n",
    "            try:\n",
    "                posts_shape_fix[i_].append(posts[j_][i_])\n",
    "            except Exception as e:  \n",
    "                print(f\"Error: {e}, current i is {i_}, current j is {j_}\")\n",
    "                print(f\"current posts[j] is {posts[j_]}\")    \n",
    "                exit(0)\n",
    "        posts = posts_shape_fix\n",
    "\n",
    "        labels = batch['type'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(posts)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "        global_step += 1\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iter {i}, train loss: {loss.item()}\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    return avg_loss, global_step\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # posts is a list of 50 strings for each of the 16 samples\n",
    "            # We need to convert it to (16, 50) to fit the model\n",
    "            posts = batch['posts']  # Shape: (50, 16)\n",
    "            real_batch_size = len(posts[0])\n",
    "            posts_shape_fix = [[] for _ in range(real_batch_size)]\n",
    "            for i, j in itertools.product(range(real_batch_size), range(50)):\n",
    "                try:\n",
    "                    posts_shape_fix[i].append(posts[j][i])\n",
    "                except Exception as e:  \n",
    "                    print(f\"Error: {e}, current i is {i}, current j is {j}\")\n",
    "                    print(f\"current posts[j] is {posts[j]}\")    \n",
    "                    exit(0)\n",
    "            posts = posts_shape_fix\n",
    "            labels = batch['type'].to(device)\n",
    "            \n",
    "            outputs = model(posts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            accuracy = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "            val_loss += loss.item()\n",
    "            val_acc += accuracy.item()\n",
    "    \n",
    "    return val_loss / len(dataloader), val_acc / len(dataloader) \n",
    "\n",
    "def train(dataset, model, device, batch_size=32, epochs=10, lr=2e-5, patience=3, scheduler_type='linear', model_save_name='best_model.pth'):\n",
    "    train_dataset, val_dataset = split_dataset(dataset)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    if scheduler_type == 'linear':\n",
    "        scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)\n",
    "    elif scheduler_type == 'cosine':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # 清空之前的日志目录\n",
    "    log_dir = 'runs/mbti_classification'\n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    model.to(device)\n",
    "    \n",
    "    global_step = 0\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, global_step = train_epoch(model, train_loader, criterion, optimizer, device, writer, global_step)\n",
    "        print(\"validating...\")\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Average train loss: {train_loss}, Average val loss: {val_loss}, Average val accuracy: {val_acc}\")   \n",
    "        \n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), f'./result/{model_save_name}')\n",
    "            print(\"Model saved!\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "# 使用示例：\n",
    "# dataset = 你的数据集\n",
    "# model = 你的模型\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# train(dataset, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RoPE.forward() missing 1 required positional argument: 'seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MBTIClassifier()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 109\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model, device, batch_size, epochs, lr, patience, scheduler_type, model_save_name)\u001b[0m\n\u001b[1;32m    106\u001b[0m epochs_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 109\u001b[0m     train_loss, global_step \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_loader, criterion, device)\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device, writer, global_step)\u001b[0m\n\u001b[1;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 102\u001b[0m, in \u001b[0;36mMBTIClassifier.forward\u001b[0;34m(self, batch_sentences_list)\u001b[0m\n\u001b[1;32m     99\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (sequence_length, batch_size, embedding_dim)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# 对句子序列进行位置编码\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(embeddings)\n\u001b[1;32m    105\u001b[0m transformer_encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(embeddings)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: RoPE.forward() missing 1 required positional argument: 'seq_len'"
     ]
    }
   ],
   "source": [
    "model = MBTIClassifier()\n",
    "train(dataset, model, torch.device('cuda'), scheduler_type='linear', epochs=100, batch_size=128, lr=1e-3, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, train loss: 1.0516107082366943\n",
      "Iter 10, train loss: 0.7230733036994934\n",
      "Iter 20, train loss: 0.8668584227561951\n",
      "Iter 30, train loss: 1.0122649669647217\n",
      "Iter 40, train loss: 0.9145032167434692\n",
      "Iter 50, train loss: 0.8512702584266663\n",
      "Iter 60, train loss: 1.331147313117981\n",
      "Iter 70, train loss: 0.885358989238739\n",
      "Iter 80, train loss: 1.171416997909546\n",
      "Iter 90, train loss: 1.0519291162490845\n",
      "Iter 100, train loss: 0.9134889245033264\n",
      "validating...\n",
      "Epoch 0, Average train loss: 0.9575668414798352, Average val loss: 1.106034361890384, Average val accuracy: 0.686224490404129\n",
      "Model saved!\n",
      "Iter 0, train loss: 1.101692795753479\n",
      "Iter 10, train loss: 1.2009905576705933\n",
      "Iter 20, train loss: 0.875921368598938\n",
      "Iter 30, train loss: 1.0464540719985962\n",
      "Iter 40, train loss: 0.7764913439750671\n",
      "Iter 50, train loss: 0.8539900779724121\n",
      "Iter 60, train loss: 0.987011194229126\n",
      "Iter 70, train loss: 0.8474991917610168\n",
      "Iter 80, train loss: 1.010483741760254\n",
      "Iter 90, train loss: 0.8990306258201599\n",
      "Iter 100, train loss: 1.0022826194763184\n",
      "validating...\n",
      "Epoch 1, Average train loss: 0.928051786138377, Average val loss: 1.1491885760000773, Average val accuracy: 0.6839923475469861\n",
      "Iter 0, train loss: 0.8919848203659058\n",
      "Iter 10, train loss: 0.9224682450294495\n",
      "Iter 20, train loss: 0.6710045337677002\n",
      "Iter 30, train loss: 0.9976544976234436\n",
      "Iter 40, train loss: 0.906955361366272\n",
      "Iter 50, train loss: 1.0704628229141235\n",
      "Iter 60, train loss: 0.8601554036140442\n",
      "Iter 70, train loss: 0.8326860666275024\n",
      "Iter 80, train loss: 0.8195202946662903\n",
      "Iter 90, train loss: 1.2079198360443115\n",
      "Iter 100, train loss: 0.8190551400184631\n",
      "validating...\n",
      "Epoch 2, Average train loss: 0.9165839481791225, Average val loss: 1.1373152605124883, Average val accuracy: 0.6935586759022304\n",
      "Model saved!\n",
      "Iter 0, train loss: 0.8744553923606873\n",
      "Iter 10, train loss: 0.6862356066703796\n",
      "Iter 20, train loss: 0.8929797410964966\n",
      "Iter 30, train loss: 0.7168558835983276\n",
      "Iter 40, train loss: 0.715664267539978\n",
      "Iter 50, train loss: 0.8615163564682007\n",
      "Iter 60, train loss: 0.9476955533027649\n",
      "Iter 70, train loss: 1.0888276100158691\n",
      "Iter 80, train loss: 1.0149965286254883\n",
      "Iter 90, train loss: 1.0042167901992798\n",
      "Iter 100, train loss: 0.6347053647041321\n",
      "validating...\n",
      "Epoch 3, Average train loss: 0.8917949735571485, Average val loss: 1.1663433696542467, Average val accuracy: 0.6806441332612719\n",
      "Iter 0, train loss: 0.7676282525062561\n",
      "Iter 10, train loss: 0.8725036382675171\n",
      "Iter 20, train loss: 0.9010945558547974\n",
      "Iter 30, train loss: 0.8657653331756592\n",
      "Iter 40, train loss: 0.926701009273529\n",
      "Iter 50, train loss: 0.9796820878982544\n",
      "Iter 60, train loss: 0.8163105249404907\n",
      "Iter 70, train loss: 0.9709057807922363\n",
      "Iter 80, train loss: 0.8212499618530273\n",
      "Iter 90, train loss: 0.9137952327728271\n",
      "Iter 100, train loss: 0.7958875298500061\n",
      "validating...\n",
      "Epoch 4, Average train loss: 0.874854876360762, Average val loss: 1.1636794890676225, Average val accuracy: 0.690130740404129\n",
      "Iter 0, train loss: 0.724755048751831\n",
      "Iter 10, train loss: 0.7994804978370667\n",
      "Iter 20, train loss: 0.8329315185546875\n",
      "Iter 30, train loss: 0.8678691387176514\n",
      "Iter 40, train loss: 0.6025170087814331\n",
      "Iter 50, train loss: 0.9540444612503052\n",
      "Iter 60, train loss: 0.7840752601623535\n",
      "Iter 70, train loss: 0.7123344540596008\n",
      "Iter 80, train loss: 0.8672436475753784\n",
      "Iter 90, train loss: 0.8086640238761902\n",
      "Iter 100, train loss: 0.8313301205635071\n",
      "validating...\n",
      "Epoch 5, Average train loss: 0.8644975117587168, Average val loss: 1.1682346016168594, Average val accuracy: 0.6952327830450875\n",
      "Model saved!\n",
      "Iter 0, train loss: 1.0248363018035889\n",
      "Iter 10, train loss: 0.7065012454986572\n",
      "Iter 20, train loss: 0.9163049459457397\n",
      "Iter 30, train loss: 0.6755592226982117\n",
      "Iter 40, train loss: 0.7516904473304749\n",
      "Iter 50, train loss: 0.8322116136550903\n",
      "Iter 60, train loss: 0.6724963784217834\n",
      "Iter 70, train loss: 0.9275869727134705\n",
      "Iter 80, train loss: 0.8090559244155884\n",
      "Iter 90, train loss: 0.6795666217803955\n",
      "Iter 100, train loss: 0.7523989081382751\n",
      "validating...\n",
      "Epoch 6, Average train loss: 0.8427746093601262, Average val loss: nan, Average val accuracy: 0.6890146689755576\n",
      "Iter 0, train loss: 0.6355583667755127\n",
      "Iter 10, train loss: 0.7407312393188477\n",
      "Iter 20, train loss: 0.7609096765518188\n",
      "Iter 30, train loss: 0.6497629284858704\n",
      "Iter 40, train loss: 0.6006477475166321\n",
      "Iter 50, train loss: 0.7674944400787354\n",
      "Iter 60, train loss: 0.793755829334259\n",
      "Iter 70, train loss: 0.9488986730575562\n",
      "Iter 80, train loss: 0.8460233211517334\n",
      "Iter 90, train loss: 0.8531139492988586\n",
      "Iter 100, train loss: 0.7868471741676331\n",
      "validating...\n",
      "Epoch 7, Average train loss: 0.8262596261610679, Average val loss: 1.1916597017220087, Average val accuracy: 0.6822385213204792\n",
      "Iter 0, train loss: 0.6066659688949585\n",
      "Iter 10, train loss: 0.7864916324615479\n",
      "Iter 20, train loss: 0.9367032051086426\n",
      "Iter 30, train loss: 0.6339202523231506\n",
      "Iter 40, train loss: 0.8760986328125\n",
      "Iter 50, train loss: 0.8975886106491089\n",
      "Iter 60, train loss: 0.9406703114509583\n",
      "Iter 70, train loss: 0.6969391703605652\n",
      "Iter 80, train loss: 0.9149655699729919\n",
      "Iter 90, train loss: 0.8213392496109009\n",
      "Iter 100, train loss: 0.8327123522758484\n",
      "validating...\n",
      "Epoch 8, Average train loss: 0.8082548363493123, Average val loss: 1.1912100591829844, Average val accuracy: 0.6890146689755576\n",
      "Iter 0, train loss: 0.642591118812561\n",
      "Iter 10, train loss: 0.626844048500061\n",
      "Iter 20, train loss: 0.9387328624725342\n",
      "Iter 30, train loss: 0.5214269161224365\n",
      "Iter 40, train loss: 1.0485166311264038\n",
      "Iter 50, train loss: 0.5834759473800659\n",
      "Iter 60, train loss: 0.6579082012176514\n",
      "Iter 70, train loss: 0.7730023264884949\n",
      "Iter 80, train loss: 0.9627630710601807\n",
      "Iter 90, train loss: 1.0401101112365723\n",
      "Iter 100, train loss: 0.7664991021156311\n",
      "validating...\n",
      "Epoch 9, Average train loss: 0.7905479081726949, Average val loss: nan, Average val accuracy: 0.6957908187593732\n",
      "Model saved!\n",
      "Iter 0, train loss: 0.6602548360824585\n",
      "Iter 10, train loss: 0.6567227244377136\n",
      "Iter 20, train loss: 0.7892186045646667\n",
      "Iter 30, train loss: 0.6995382308959961\n",
      "Iter 40, train loss: 1.0927456617355347\n",
      "Iter 50, train loss: 0.8899015188217163\n",
      "Iter 60, train loss: 0.7342398762702942\n",
      "Iter 70, train loss: 0.7451837062835693\n",
      "Iter 80, train loss: 0.7981590628623962\n",
      "Iter 90, train loss: 0.7935364246368408\n",
      "Iter 100, train loss: 0.7162022590637207\n",
      "validating...\n",
      "Epoch 10, Average train loss: 0.769967398512254, Average val loss: 1.2249495834112167, Average val accuracy: 0.6906887761184147\n",
      "Iter 0, train loss: 0.938571572303772\n",
      "Iter 10, train loss: 0.7624416351318359\n",
      "Iter 20, train loss: 0.8402783274650574\n",
      "Iter 30, train loss: 0.8789746761322021\n",
      "Iter 40, train loss: 0.7964718341827393\n",
      "Iter 50, train loss: 0.6862171292304993\n",
      "Iter 60, train loss: 0.9057322144508362\n",
      "Iter 70, train loss: 0.7175365686416626\n",
      "Iter 80, train loss: 0.6799308061599731\n",
      "Iter 90, train loss: 0.7218530774116516\n",
      "Iter 100, train loss: 0.7143487334251404\n",
      "validating...\n",
      "Epoch 11, Average train loss: 0.7481037184185938, Average val loss: 1.2154588954789298, Average val accuracy: 0.697943240404129\n",
      "Model saved!\n",
      "Iter 0, train loss: 0.7296460866928101\n",
      "Iter 10, train loss: 1.1708965301513672\n",
      "Iter 20, train loss: 0.4586736559867859\n",
      "Iter 30, train loss: 0.7092859148979187\n",
      "Iter 40, train loss: 0.9661741256713867\n",
      "Iter 50, train loss: 0.8154000639915466\n",
      "Iter 60, train loss: 0.6175956130027771\n",
      "Iter 70, train loss: 0.9577767848968506\n",
      "Iter 80, train loss: 0.828693151473999\n",
      "Iter 90, train loss: 0.8427931070327759\n",
      "Iter 100, train loss: 0.6577539443969727\n",
      "validating...\n",
      "Epoch 12, Average train loss: 0.7169064731772886, Average val loss: 1.2579278200864792, Average val accuracy: 0.6923628832612719\n",
      "Iter 0, train loss: 0.9044861197471619\n",
      "Iter 10, train loss: 0.7507719397544861\n",
      "Iter 20, train loss: 0.554077684879303\n",
      "Iter 30, train loss: 0.6089123487472534\n",
      "Iter 40, train loss: 0.8117232322692871\n",
      "Iter 50, train loss: 0.769467830657959\n",
      "Iter 60, train loss: 0.5837345123291016\n",
      "Iter 70, train loss: 0.6540120840072632\n",
      "Iter 80, train loss: 0.5020268559455872\n",
      "Iter 90, train loss: 0.5818287134170532\n",
      "Iter 100, train loss: 0.7299116253852844\n",
      "validating...\n",
      "Epoch 13, Average train loss: 0.7010624597378827, Average val loss: 1.2769844297851836, Average val accuracy: 0.6969068901879447\n",
      "Iter 0, train loss: 0.582535982131958\n",
      "Iter 10, train loss: 0.6654395461082458\n",
      "Iter 20, train loss: 0.6460644602775574\n",
      "Iter 30, train loss: 0.8640066385269165\n",
      "Iter 40, train loss: 0.6746728420257568\n",
      "Iter 50, train loss: 0.505305290222168\n",
      "Iter 60, train loss: 0.7375167012214661\n",
      "Iter 70, train loss: 0.7305002212524414\n",
      "Iter 80, train loss: 0.8155776262283325\n",
      "Iter 90, train loss: 0.5494097471237183\n",
      "Iter 100, train loss: 0.6604552865028381\n",
      "validating...\n",
      "Epoch 14, Average train loss: 0.6761562315695876, Average val loss: 1.299804048878806, Average val accuracy: 0.6811224498919078\n",
      "Iter 0, train loss: 0.7144620418548584\n",
      "Iter 10, train loss: 0.5819584131240845\n",
      "Iter 20, train loss: 0.8759850859642029\n",
      "Iter 30, train loss: 0.8809057474136353\n",
      "Iter 40, train loss: 0.7037091255187988\n",
      "Iter 50, train loss: 0.6757221221923828\n",
      "Iter 60, train loss: 0.7277158498764038\n",
      "Iter 70, train loss: 0.6018880605697632\n",
      "Iter 80, train loss: 0.6204270124435425\n",
      "Iter 90, train loss: 0.808139443397522\n",
      "Iter 100, train loss: 0.8296977281570435\n",
      "validating...\n",
      "Epoch 15, Average train loss: 0.6549639299922033, Average val loss: 1.3064586839505605, Average val accuracy: 0.6974649259022304\n",
      "Iter 0, train loss: 0.6003491878509521\n",
      "Iter 10, train loss: 0.8071244359016418\n",
      "Iter 20, train loss: 0.6680736541748047\n",
      "Iter 30, train loss: 0.6791886687278748\n",
      "Iter 40, train loss: 0.5742892622947693\n",
      "Iter 50, train loss: 0.6273171305656433\n",
      "Iter 60, train loss: 0.48686519265174866\n",
      "Iter 70, train loss: 0.72431880235672\n",
      "Iter 80, train loss: 0.5854883193969727\n",
      "Iter 90, train loss: 0.5915679931640625\n",
      "Iter 100, train loss: 0.5251122713088989\n",
      "validating...\n",
      "Epoch 16, Average train loss: 0.6240196416684247, Average val loss: 1.3341350640569414, Average val accuracy: 0.6890146689755576\n",
      "Iter 0, train loss: 0.7912559509277344\n",
      "Iter 10, train loss: 0.7081930041313171\n",
      "Iter 20, train loss: 0.575276255607605\n",
      "Iter 30, train loss: 0.5520259737968445\n",
      "Iter 40, train loss: 0.9147665500640869\n",
      "Iter 50, train loss: 0.7685251832008362\n",
      "Iter 60, train loss: 0.6557441353797913\n",
      "Iter 70, train loss: 0.6833263039588928\n",
      "Iter 80, train loss: 0.6822205781936646\n",
      "Iter 90, train loss: 0.7522572875022888\n",
      "Iter 100, train loss: 0.5165166854858398\n",
      "validating...\n",
      "Epoch 17, Average train loss: 0.6031040942997014, Average val loss: 1.34925890820367, Average val accuracy: 0.6812021689755576\n",
      "Iter 0, train loss: 0.6033542156219482\n",
      "Iter 10, train loss: 0.44562044739723206\n",
      "Iter 20, train loss: 0.6827983856201172\n",
      "Iter 30, train loss: 0.5082485675811768\n",
      "Iter 40, train loss: 0.6907302141189575\n",
      "Iter 50, train loss: 0.3050883412361145\n",
      "Iter 60, train loss: 0.42356038093566895\n",
      "Iter 70, train loss: 0.4640156328678131\n",
      "Iter 80, train loss: 0.7181417942047119\n",
      "Iter 90, train loss: 0.7080749869346619\n",
      "Iter 100, train loss: 0.6239838600158691\n",
      "validating...\n",
      "Epoch 18, Average train loss: 0.5805554280587293, Average val loss: 1.3861078577382224, Average val accuracy: 0.682318240404129\n",
      "Iter 0, train loss: 0.4373464286327362\n",
      "Iter 10, train loss: 0.5007201433181763\n",
      "Iter 20, train loss: 0.6537701487541199\n",
      "Iter 30, train loss: 0.582852303981781\n",
      "Iter 40, train loss: 0.4661369323730469\n",
      "Iter 50, train loss: 0.3744349181652069\n",
      "Iter 60, train loss: 0.4542025327682495\n",
      "Iter 70, train loss: 0.4462267756462097\n",
      "Iter 80, train loss: 0.4566619098186493\n",
      "Iter 90, train loss: 0.6520535349845886\n",
      "Iter 100, train loss: 0.40778395533561707\n",
      "validating...\n",
      "Epoch 19, Average train loss: 0.5529074176735834, Average val loss: nan, Average val accuracy: 0.6867825261184147\n",
      "Iter 0, train loss: 0.5759447813034058\n",
      "Iter 10, train loss: 0.36163070797920227\n",
      "Iter 20, train loss: 0.6695898771286011\n",
      "Iter 30, train loss: 0.649878203868866\n",
      "Iter 40, train loss: 0.4720081388950348\n",
      "Iter 50, train loss: 0.5636144280433655\n",
      "Iter 60, train loss: 0.5128310918807983\n",
      "Iter 70, train loss: 0.43872547149658203\n",
      "Iter 80, train loss: 0.6935619711875916\n",
      "Iter 90, train loss: 0.6246117949485779\n",
      "Iter 100, train loss: 0.68440181016922\n",
      "validating...\n",
      "Epoch 20, Average train loss: 0.5288757845896099, Average val loss: 1.4137875735759735, Average val accuracy: 0.6828762761184147\n",
      "Iter 0, train loss: 0.3699146509170532\n",
      "Iter 10, train loss: 0.6929856538772583\n",
      "Iter 20, train loss: 0.42120248079299927\n",
      "Iter 30, train loss: 0.37190577387809753\n",
      "Iter 40, train loss: 0.49431201815605164\n",
      "Iter 50, train loss: 0.7462667226791382\n",
      "Iter 60, train loss: 0.524312436580658\n",
      "Iter 70, train loss: 0.40298357605934143\n",
      "Iter 80, train loss: 0.5606215000152588\n",
      "Iter 90, train loss: 0.5229495167732239\n",
      "Iter 100, train loss: 0.5483992695808411\n",
      "validating...\n",
      "Epoch 21, Average train loss: 0.5082148510381717, Average val loss: 1.4286434480122157, Average val accuracy: 0.6906887761184147\n",
      "Iter 0, train loss: 0.4049087166786194\n",
      "Iter 10, train loss: 0.43323397636413574\n",
      "Iter 20, train loss: 0.5738059878349304\n",
      "Iter 30, train loss: 0.46404141187667847\n",
      "Iter 40, train loss: 0.4017443358898163\n",
      "Iter 50, train loss: 0.4769120216369629\n",
      "Iter 60, train loss: 0.43627113103866577\n",
      "Iter 70, train loss: 0.6439821124076843\n",
      "Iter 80, train loss: 0.2830907106399536\n",
      "Iter 90, train loss: 0.6552835702896118\n"
     ]
    }
   ],
   "source": [
    "model = MBTIClassifier()\n",
    "model.load_state_dict(torch.load('./result/best_model.pth'))\n",
    "train(dataset, model, torch.device('cuda'), scheduler_type='cosine', epochs=100, batch_size=64, lr=5e-5, patience=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, device, batch_size=32):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    total_acc = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # posts is a list of 50 strings for each of the 16 samples\n",
    "            # We need to convert it to (16, 50) to fit the model\n",
    "            posts = batch['posts']  # Shape: (50, 16)\n",
    "            real_batch_size = len(posts[0])\n",
    "            posts_shape_fix = [[] for _ in range(real_batch_size)]\n",
    "            for i, j in itertools.product(range(real_batch_size), range(50)):\n",
    "                try:\n",
    "                    posts_shape_fix[i].append(posts[j][i])\n",
    "                except Exception as e:  \n",
    "                    print(f\"Error: {e}, current i is {i}, current j is {j}\")\n",
    "                    print(f\"current posts[j] is {posts[j]}\")    \n",
    "                    exit(0)\n",
    "            posts = posts_shape_fix\n",
    "            labels = batch['type'].to(device)\n",
    "            \n",
    "            outputs = model(posts)\n",
    "            accuracy = (outputs.argmax(dim=1) == labels).float().sum().item()\n",
    "            total_acc += accuracy\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    return total_acc / total_samples\n",
    "\n",
    "# 使用示例：\n",
    "# dataset = 你的数据集\n",
    "# model = 训练好的模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy = evaluate(model, dataset, device)\n",
    "print(f\"Model accuracy on the entire dataset: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
